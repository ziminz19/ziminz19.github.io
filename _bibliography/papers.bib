---
---

@inproceedings{agarwal2025the,
  title     = {The Unreasonable Effectiveness of Entropy Minimization in {LLM} Reasoning},
  author    = {Shivam Agarwal* and Zimin Zhang* and Lifan Yuan and Jiawei Han and Hao Peng},
  booktitle = {The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2025},
  url       = {https://openreview.net/forum?id=UfFTBEsLgI},
  selected  = {true}
}

@inproceedings{zhang-etal-2025-automated,
  title     = {Automated Molecular Concept Generation and Labeling with Large Language Models},
  author    = {Zhang, Zimin  and
               Wu, Qianli  and
               Xia, Botao  and
               Sun, Fang  and
               Hu, Ziniu  and
               Sun, Yizhou  and
               Zhang, Shichang},
  booktitle = {Proceedings of the 31st International Conference on Computational Linguistics (COLING)},
  month     = jan,
  year      = {2025},
  address   = {Abu Dhabi, UAE},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.coling-main.462/},
  pages     = {6918--6936},
  abstract  = {Artificial intelligence (AI) is transforming scientific research, with explainable AI methods like concept-based models (CMs) showing promise for new discoveries. However, in molecular science, CMs are less common than black-box models like Graph Neural Networks (GNNs), due to their need for predefined concepts and manual labeling. This paper introduces the Automated Molecular Concept (AutoMolCo) framework, which leverages Large Language Models (LLMs) to automatically generate and label predictive molecular concepts. Through iterative concept refinement, AutoMolCo enables simple linear models to outperform GNNs and LLM in-context learning on several benchmarks. The framework operates without human knowledge input, overcoming limitations of existing CMs while maintaining explainability and allowing easy intervention. Experiments on MoleculeNet and High-Throughput Experimentation (HTE) datasets demonstrate that AutoMolCoinduced explainable CMs are beneficial for molecular science research.},
  selected  = {true}
}

@misc{wang2024officebenchbenchmarkinglanguageagents,
  title         = {OfficeBench: Benchmarking Language Agents across Multiple Applications for Office Automation},
  author        = {Zilong Wang and Yuedong Cui and Li Zhong and Zimin Zhang and Da Yin and Bill Yuchen Lin and Jingbo Shang},
  year          = {2024},
  eprint        = {2407.19056},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2407.19056}
}

@inproceedings{du2025how,
  title     = {How Post-Training Reshapes {LLM}s: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence},
  author    = {Hongzhe Du and Weikai Li and Min Cai and Karim Saraipour and Zimin Zhang and Himabindu Lakkaraju and Yizhou Sun and Shichang Zhang},
  booktitle = {Second Conference on Language Modeling (COLM)},
  year      = {2025},
  url       = {https://openreview.net/forum?id=w5DSwn9wTC}
}